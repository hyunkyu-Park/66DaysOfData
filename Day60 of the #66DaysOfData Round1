ㆍTwo other ways to prevent overfitting are 'regularization' and 'drop out'.  
As I studied yesterday, complex models are more likely to be overfitting than simple models.  
Then, what are complex models? The complexity of the model is decided by number of hidden layers and parameters.  
The more these variables, the more they fall into the curse of dimensionality. One of ways to make complex models simpler is 'regularization'.  
Regularization takes advantage of all features, but it is a way to solve them by reducing the value and size of the parameters.  
ㆍDropout is a way to avoid using some neural networks in the learning process.  
For example, if the Dropout ratio is 0.5, you're going to randomly use half a neuron for each course of study.  
For Dropout, it is common to use only for neural network training and not for prediction.  
It prevents overfitting by preventing learning from being dependent on specific neurons or combinations.  
It also gives an ensemble effect because it randomly selects and learns neurons according to a given proportion.
