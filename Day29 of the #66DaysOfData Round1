ㆍStudied basic algorithms called 'built in functions', 'itertools', 'heapq'. I am going to implement 'bisect' and 'collections tomorrow'
ㆍFollowing yesterday, I tried to understand Attention model, self-attention model, and multi-head self attention model. 
These are necessary concepts to understand bert. What is bert, and why does bert has been developed. The more I study NLP, I think there is a lot of potential.  
ㆍStarted to make my presentation for NLP, bert and to organize that I am going to speech.  
